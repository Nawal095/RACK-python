{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing all necessary modules for \"ConnectionManager\" class\n",
    "import sqlite3\n",
    "\n",
    "# importing all necessary modules for \"CosineSimilarityMeasure\" class\n",
    "import traceback\n",
    "import logging\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# importing all necessary modules for \"TokenStemmer\" class\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# importing all necessary modules and resources for \"StopWordRemover\" class\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# importing all necessary modules for \"LexicalSimilarityProvider\" class\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Data\n",
    "\n",
    "MAXAPI = 10\n",
    "DELTA1 = 10\n",
    "DELTA2 = 10\n",
    "alpha = 0.325\n",
    "beta = 0.575\n",
    "psi = 0.10\n",
    "gamma = 0\n",
    "GOLDSET_SIZE = 10\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Word Remover\n",
    "\n",
    "class StopWordRemover:\n",
    "\n",
    "    def getRefinedSentence(self, text):\n",
    "        # punctuation removed\n",
    "        new_string = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # stop word removed\n",
    "        text_tokens = word_tokenize(new_string)\n",
    "        tokens_without_sw = [word.strip() for word in text_tokens if not word in stopwords.words()]\n",
    "        return ' '.join(tokens_without_sw)\n",
    "    \n",
    "    def removeStopWords(self, text_tokens):\n",
    "        tokens_without_sw = [word.strip() for word in text_tokens if not word in stopwords.words()]\n",
    "        return tokens_without_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Stemmer\n",
    "\n",
    "class TokenStemmer:\n",
    "\n",
    "    def performStemming(self, args):\n",
    "        if type(args) == list:\n",
    "            return [stemmer.stem(token) for token in args]\n",
    "        elif type(args) == str:\n",
    "            return stemmer.stem(args[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Sorter\n",
    "\n",
    "class ItemSorter:\n",
    "    \n",
    "    def sortHashMapInt(self, wordMap):\n",
    "        return {k: v for k, v in sorted(wordMap.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    def sortHashMapDouble(self, wordMap):\n",
    "        return {k: v for k, v in sorted(wordMap.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Token\n",
    "\n",
    "class APIToken:\n",
    "    def __init__(self):\n",
    "        self.token = ''\n",
    "        self.KACScore = 0\n",
    "        self.KKCScore = 0\n",
    "        self.KPACScore = 0\n",
    "        self.totalScore = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity Measure\n",
    "\n",
    "class CosineSimilarityMeasure:\n",
    "    \n",
    "    def __init__(self, first, second):\n",
    "        self.sqliteConnection = None\n",
    "        self.first = first\n",
    "        self.second = second\n",
    "    \n",
    "    def getCosineSimilarityScore(self):\n",
    "        if type(self.first) == str and type(self.second) == str:\n",
    "            return model.similarity(self.first, self.second)\n",
    "        \n",
    "        elif type(self.first) == list and type(self.second) == list:\n",
    "            c1 = Counter(self.first)\n",
    "            c2 = Counter(self.second)\n",
    "\n",
    "            terms = set(c1).union(c2)\n",
    "            dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "            magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "            magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "            return dotprod / (magA * magB)\n",
    "        \n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection Manager\n",
    "\n",
    "class ConnectionManager:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sqliteConnection = None\n",
    "    \n",
    "    def getConnection(self):\n",
    "        try:\n",
    "            self.sqliteConnection = sqlite3.connect('RACK-EMSE.db')\n",
    "            cursor = self.sqliteConnection.cursor()\n",
    "            print(\"Database created and Successfully Connected to SQLite\")\n",
    "\n",
    "            sqlite_select_Query = \"select sqlite_version();\"\n",
    "            cursor.execute(sqlite_select_Query)\n",
    "            record = cursor.fetchall()\n",
    "            print(\"SQLite Database Version is: \", record)\n",
    "            cursor.close()\n",
    "            \n",
    "            return self.sqliteConnection\n",
    "\n",
    "        except sqlite3.Error as error:\n",
    "            print(\"Error while connecting to sqlite\", error)\n",
    "            \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency Score Provider\n",
    "\n",
    "class AdjacencyScoreProvider:\n",
    "    \n",
    "    adjacencymap = {}\n",
    "    \n",
    "    def __init__(self, queryTerms):\n",
    "        self.queryTerms = queryTerms\n",
    "        self.keys = []\n",
    "        self.simscores = []\n",
    "    \n",
    "    def collectAdjacentTerms(self):\n",
    "        try:\n",
    "            conn = ConnectionManager()\n",
    "            sqliteConnection = conn.getConnection()\n",
    "            \n",
    "            if sqliteConnection != None:\n",
    "                for key in self.queryTerms:\n",
    "                    cursor = sqliteConnection.cursor()\n",
    "\n",
    "                    sqlite_select_Query = \"select distinct Token from TextToken where EntryID in (select EntryID from TextToken where Token='\" + key + \"') and Token!='\" + key + \"'\"\n",
    "                    cursor.execute(sqlite_select_Query)\n",
    "                    record = cursor.fetchall()                    \n",
    "                    \n",
    "                    adjacent = []\n",
    "                    \n",
    "                    for rec in record:\n",
    "                        adjacent.append(rec[0])\n",
    "                        \n",
    "                    self.adjacencymap[key] = adjacent\n",
    "                \n",
    "                self.keys = list(self.adjacencymap.keys())\n",
    "                sqliteConnection.close()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(traceback.format_exc())\n",
    "            \n",
    "    def collectAdjacencyScores(self):\n",
    "        self.keys = list(self.adjacencymap.keys())\n",
    "        dimension = len(self.keys)\n",
    "        \n",
    "        for i in range(dimension):\n",
    "            self.simscores.append([-1]*dimension) \n",
    "        \n",
    "        for i in range(len(self.keys)):\n",
    "            first = self.keys[i]\n",
    "            for j in range(len(self.keys)):\n",
    "                if j > i:\n",
    "                    second = self.keys[j]\n",
    "                    cos = CosineSimilarityMeasure(first, second)\n",
    "                    simscore = cos.getCosineSimilarityScore()\n",
    "                    self.simscores[i][j] = simscore\n",
    "                    self.simscores[j][i] = simscore\n",
    "                                \n",
    "        return self.simscores\n",
    "    \n",
    "    def getQueryTermAdjacencyScores(self):\n",
    "        self.collectAdjacentTerms()\n",
    "        print(self.collectAdjacencyScores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Similarity Provider\n",
    "\n",
    "class LexicalSimilarityProvider:\n",
    "    \n",
    "    def __init__(self, queryTerms, candidates):\n",
    "        self.queryTerms = queryTerms\n",
    "        self.candidates = candidates\n",
    "        self.simScoreMap = {}\n",
    "        \n",
    "    def decomposeCamelCase(self, token):\n",
    "        return re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', token)).split()\n",
    "    \n",
    "    def clearTheTokens(self, tokenParts):\n",
    "        refined = StopWordRemover.removeStopWords(tokenParts)\n",
    "        stemmed = TokenStemmer.performStemming(refined)\n",
    "        return stemmed\n",
    "    \n",
    "    def normalizeAPIToken(self, apiToken):\n",
    "        # normalize the API token into granular tokens\n",
    "        decomposed = decomposeCamelCase(apiToken)\n",
    "        normalized = clearTheTokens(decomposed)\n",
    "        return normalized\n",
    "    \n",
    "    def getLexicalSimilarityScores(self):\n",
    "        for apiName in self.candidates:\n",
    "            normalizedTokens = normalizeAPIToken(apiName)\n",
    "            cosMeasure = CosineSimilarityMeasure(normalizedTokens, self.candidates)\n",
    "            simScore = cosMeasure.getCosineSimilarityScore()\n",
    "            if apiName not in self.simScoreMap.keys():\n",
    "                self.simScoreMap[apiName] = simScore\n",
    "            return self.simScoreMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant API Collector\n",
    "\n",
    "class RelevantAPICollector:\n",
    "    \n",
    "    def __init__(self, queryTerms):\n",
    "        self.queryTerms = queryTerms\n",
    "        \n",
    "    def collectAPIsforQuery(self):\n",
    "        tokenmap = {}\n",
    "        try:\n",
    "            conn = ConnectionManager()\n",
    "            sqliteConnection = conn.getConnection()\n",
    "            \n",
    "            if sqliteConnection != None:\n",
    "                for texttoken in self.queryTerms:\n",
    "                    cursor = sqliteConnection.cursor()\n",
    "\n",
    "                    sqlite_select_Query = \"select ct.Token from CodeToken as ct, TextToken as tt where ct.EntryID=tt.EntryID and tt.Token='\"+ texttoken + \"' group by ct.Token order by count(*) desc limit \"+ str(DELTA1);\n",
    "                    cursor.execute(sqlite_select_Query)\n",
    "                    results = cursor.fetchall()\n",
    "                    \n",
    "                    apis = []\n",
    "                    \n",
    "                    for res in results:\n",
    "                        apis.append(res[0])\n",
    "                        \n",
    "                    tokenmap[texttoken] = apis\n",
    "                \n",
    "                sqliteConnection.close()\n",
    "                return tokenmap\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(traceback.format_exc())\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coocurrence Score Provider\n",
    "\n",
    "class CoocurrenceScoreProvider:\n",
    "    \n",
    "    def __init__(self, queryTerms):\n",
    "        self.queryTerms = queryTerms\n",
    "        self.keys = list(set(queryTerms))\n",
    "        self.coocAPIMap = {}\n",
    "        self.coocScoreMap = {}\n",
    "        \n",
    "    def getKeyPairs(self):\n",
    "        temp = []\n",
    "        for i in range(len(self.keys)):\n",
    "            first = self.keys[i]\n",
    "            for j in range(i + 1,  len(self.keys)):\n",
    "                second = self.keys[j]\n",
    "                keypair = first + \"-\" + second\n",
    "                temp.append(keypair)\n",
    "        return temp\n",
    "    \n",
    "    def collectCoocAPIs(self, keypairs):\n",
    "        try:\n",
    "            conn = ConnectionManager()\n",
    "            sqliteConnection = conn.getConnection()\n",
    "            \n",
    "            if sqliteConnection != None:\n",
    "                for keypair in keypairs:\n",
    "                    cursor = sqliteConnection.cursor()\n",
    "                    \n",
    "                    parts = keypair.split(\"-\")\n",
    "                    first = parts[0]\n",
    "                    second = parts[1]\n",
    "\n",
    "                    sqlite_select_Query = \"select Token from CodeToken where EntryID in(select EntryID from TextToken where Token='\"+ first + \"' intersect select EntryID from TextToken where Token='\"+ second + \"') group by Token order by count(*) desc limit \" + str(DELTA1);\n",
    "                    \n",
    "                    cursor.execute(sqlite_select_Query)\n",
    "                    results = cursor.fetchall()\n",
    "                    \n",
    "                    temp = []\n",
    "                    \n",
    "                    for res in results:\n",
    "                        temp.append(res[0])\n",
    "                        \n",
    "                    self.coocAPIMap[keypair] = temp\n",
    "                \n",
    "                sqliteConnection.close()\n",
    "                return self.coocAPIMap\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(traceback.format_exc())\n",
    "            return None\n",
    "        \n",
    "    def generateCoocScores(self):\n",
    "        keySet = list(self.coocAPIMap.keys())\n",
    "        for keypair in keySet:\n",
    "            apis = self.coocAPIMap[keypair]\n",
    "            length = len(apis)\n",
    "            for i in range(length):\n",
    "                score = 1 - i / length\n",
    "                api = apis[i]\n",
    "                if api in self.coocScoreMap.keys():\n",
    "                    newScore = self.coocScoreMap[api] + score;\n",
    "                    self.coocScoreMap[api] = newScore\n",
    "                else:\n",
    "                    self.coocScoreMap[api] = score\n",
    "                    \n",
    "    def normalizeScores(self):\n",
    "        maxScore = 0\n",
    "        for api in list(self.coocScoreMap.keys()):\n",
    "            score = self.coocScoreMap[api]\n",
    "            if score > maxScore:\n",
    "                maxScore = score\n",
    "        \n",
    "        for api in list(self.coocScoreMap.keys()):\n",
    "            nScore = self.coocScoreMap[api] / maxScore\n",
    "            self.coocScoreMap[api] = nScore\n",
    "            \n",
    "    def getCoocScores(self):\n",
    "        keypairs = self.getKeyPairs()\n",
    "        self.collectCoocAPIs(keypairs)\n",
    "        self.generateCoocScores()\n",
    "        self.normalizeScores()\n",
    "        return self.coocScoreMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Token Provider\n",
    "\n",
    "class CodeTokenProvider:\n",
    "    \n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "        self.tokenScoreMap = {}\n",
    "        self.stemmedQuery = []\n",
    "        self.KACMap = {}\n",
    "        self.KPACMap = {}\n",
    "        self.KKCMap = {}\n",
    "        \n",
    "    def decomposeQueryTerms(self):\n",
    "        tempQuery = self.query.lower()\n",
    "        swr = StopWordRemover()\n",
    "        tempQuery = swr.getRefinedSentence(tempQuery)\n",
    "        tokens = word_tokenize(tempQuery)\n",
    "        refined = swr.removeStopWords(tokens)\n",
    "        \n",
    "        ts = TokenStemmer()\n",
    "        stemmed = ts.performStemming(refined)\n",
    "        stemmedQuery = []\n",
    "        for token in stemmed:\n",
    "            if token.isnumeric() or len(token) <= 0:\n",
    "                continue\n",
    "            else:\n",
    "                stemmedQuery.append(token)\n",
    "                \n",
    "        self.stemmedQuery = stemmedQuery\n",
    "        return stemmedQuery\n",
    "\n",
    "    def collectTokenScores(self, queryTerms):\n",
    "        adjacent = AdjacencyScoreProvider(queryTerms)\n",
    "        adjacent.collectAdjacentTerms() # self.adjacencymap was created\n",
    "        simscores = adjacent.collectAdjacencyScores()\n",
    "        keys = list(adjacent.keys)\n",
    "        \n",
    "        collector = RelevantAPICollector(queryTerms)\n",
    "        tokenmap = collector.collectAPIsforQuery()\n",
    "        \n",
    "        self.tokenScoreMap = {}\n",
    "        \n",
    "        # KAC scores\n",
    "        self.addAssociationFrequencyScores(tokenmap)\n",
    "        # KKC scores\n",
    "        self.addTokenSimilarityScores(keys, simscores, tokenmap)\n",
    "        # KPAC scores\n",
    "        self.addDirectCoocScores()\n",
    "        # add the textual similarity scores\n",
    "        self.addExtraLayerScoreComputation()\n",
    "        \n",
    "    def collectTokenScoresKAC(self, queryTerms):\n",
    "        # collecting token scores based on KAC\n",
    "        collector = RelevantAPICollector(queryTerms)\n",
    "        tokenmap = collector.collectAPIsforQuery()\n",
    "        \n",
    "        self.tokenScoreMap = {}\n",
    "        \n",
    "        # now add the scores\n",
    "        self.addAssociationFrequencyScores(tokenmap)\n",
    "        \n",
    "        \n",
    "    def collectTokenScoresKKC(self, queryTerms):\n",
    "        # collecting scores based on AAC\n",
    "        adjacent = AdjacencyScoreProvider(queryTerms)\n",
    "        # adjacency scores\n",
    "        adjacent.collectAdjacentTerms()\n",
    "        simscores = adjacent.collectAdjacencyScores()\n",
    "        keys = list(adjacent.keys) # keys from queries\n",
    "        \n",
    "        collector = RelevantAPICollector(queryTerms)\n",
    "        tokenmap = collector.collectAPIsforQuery()\n",
    "        \n",
    "        self.tokenScoreMap = {}\n",
    "        \n",
    "        # now add the scores\n",
    "        self.addTokenSimilarityScores(keys, simscores, tokenmap)\n",
    "        \n",
    "    def collectTokenScoresKPAC(self, tokenmap):\n",
    "        self.tokenScoreMap = {}\n",
    "        self.addDirectCoocScores()\n",
    "        \n",
    "    def addTokenSimilarityScores(self, keys, simscores, tokenmap):\n",
    "        for i in range(len(keys)):\n",
    "            first = keys[i]\n",
    "            firstapi = tokenmap[first]\n",
    "            for j in range(i+1, len(keys)):\n",
    "                second = keys[j]\n",
    "                secondapi = tokenmap[second]\n",
    "                common = self.intersect(firstapi, secondapi)\n",
    "                simscore = simscores[i][j]\n",
    "                \n",
    "                if simscore > gamma:\n",
    "                    for token in common:\n",
    "                        if token in self.tokenScoreMap.keys():\n",
    "                            newOldScore = self.tokenScoreMap[token] + simscore\n",
    "                            self.tokenScoreMap[token] = newOldScore\n",
    "                        else:\n",
    "                            self.tokenScoreMap[token] = simscore\n",
    "\n",
    "                        # adding to the extra map\n",
    "                        if token in self.KKCMap.keys():\n",
    "                            newOldScore = self.KKCMap[token] + simscore\n",
    "                            self.KKCMap[token] = newOldScore\n",
    "                        else:\n",
    "                            self.KKCMap[token] = simscore\n",
    "        \n",
    "    def addAssociationFrequencyScores(self, tokenmap):\n",
    "        # association frequency score between text token and code token\n",
    "        for key in list(tokenmap.keys()):\n",
    "            apis = tokenmap[key]\n",
    "            \n",
    "            length = len(apis)\n",
    "            \n",
    "            for i in range(len(apis)):\n",
    "                \n",
    "                # now determine the score\n",
    "                score = 1 - i / length;\n",
    "\n",
    "                # add the weight\n",
    "                # score = score * StaticData.alpha\n",
    "                \n",
    "                api = apis[i]\n",
    "                # now check the score for each API\n",
    "                # add the score to the map\n",
    "                if api in self.tokenScoreMap.keys():\n",
    "                    newScore = self.tokenScoreMap[api] + score\n",
    "                    self.tokenScoreMap[api] = newScore\n",
    "                else:\n",
    "                    self.tokenScoreMap[api] = score\n",
    "                    \n",
    "                # adding scores to the extra map\n",
    "                if api in self.KACMap.keys():\n",
    "                    newScore = self.KACMap[api] + score\n",
    "                    self.KACMap[api] = newScore\n",
    "                else:\n",
    "                    self.KACMap[api] = score\n",
    "        \n",
    "    def addDirectCoocScores(self):\n",
    "        # adding direct cooccurrence scores\n",
    "        coocProvider = CoocurrenceScoreProvider(self.stemmedQuery)\n",
    "        coocScoreMap = coocProvider.getCoocScores()\n",
    "        for apiKey in list(coocScoreMap.keys()):\n",
    "            \n",
    "            coocScore = coocScoreMap[apiKey]\n",
    "            \n",
    "            # add the weight\n",
    "            # coocScore = coocScore * StaticData.beta\n",
    "            \n",
    "            # adding to the token map\n",
    "            # adding to the token score map\n",
    "            if apiKey in list(self.tokenScoreMap.keys()):\n",
    "                newScore = self.tokenScoreMap[apiKey] + coocScore\n",
    "                self.tokenScoreMap[apiKey] = newScore\n",
    "            else:\n",
    "                self.tokenScoreMap[apiKey] = coocScore\n",
    "                \n",
    "            # adding to the extra map\n",
    "            if apiKey in list(self.KPACMap.keys()):\n",
    "                newScore = self.KPACMap[apiKey] + coocScore\n",
    "                self.KPACMap[apiKey] = newScore\n",
    "            else:\n",
    "                self.KPACMap[apiKey] = coocScore\n",
    "        \n",
    "    def intersect(self, s1, s2):\n",
    "        # intersecting the two sets / list of items\n",
    "        common = [value for value in s1 if value in s2]\n",
    "        return common\n",
    "        \n",
    "    def rankAPIElements(self, scoreMap = None): # overloaded method\n",
    "        # rank the API names\n",
    "        isort = ItemSorter()\n",
    "        if scoreMap == None:\n",
    "            sorted = isort.sortHashMapDouble(self.tokenScoreMap)\n",
    "        else:\n",
    "            sorted = isort.sortHashMapDouble(scoreMap)\n",
    "            \n",
    "        rankedAPIs = []\n",
    "        for k, v in sorted.items():\n",
    "            rankedAPIs.append(k)\n",
    "        \n",
    "        topRanked = [value.strip() for value in rankedAPIs if value.strip() != '']\n",
    "        \n",
    "        # returning the ranked APIs\n",
    "        if scoreMap == None:\n",
    "            return rankedAPIs\n",
    "        else:\n",
    "            return topRanked[:MAXAPI]\n",
    "        \n",
    "    def addExtraLayerScoreComputation(self):\n",
    "        kacs = self.rankAPIElements(self.KACMap)\n",
    "        kacScoreMap = self.getNormScore(kacs)\n",
    "        \n",
    "        kpacs = self.rankAPIElements(self.KPACMap)\n",
    "        kpacScoreMap = self.getNormScore(kpacs)\n",
    "        \n",
    "        kkcs = self.rankAPIElements(self.KKCMap)\n",
    "        kkcScoreMap = self.getNormScore(kkcs)\n",
    "        \n",
    "        self.addCombinedRankingsV2(kacScoreMap, kpacScoreMap, kkcScoreMap, alpha, beta, psi)\n",
    "        \n",
    "    def addCombinedRankings(self, kacMap, kpacMap, kkcMap, alpha1, beta1, psi1):\n",
    "        # get the combined rankings\n",
    "        self.tokenScoreMap = {}\n",
    "        # HashMap<String, Double> tokenScoreMap = new HashMap<>()\n",
    "        for key in kacMap.keys():\n",
    "            score = kacMap[key]\n",
    "            score = score * alpha1\n",
    "            if key in list(self.tokenScoreMap.keys()):\n",
    "                newScore = self.tokenScoreMap[key] + score\n",
    "                self.tokenScoreMap[key] = newScore\n",
    "            else:\n",
    "                self.tokenScoreMap[key] = score\n",
    "                \n",
    "        for key in list(kpacMap.keys()):\n",
    "            score = kpacMap[key]\n",
    "            score = score * beta1\n",
    "            if key in list(self.tokenScoreMap.keys()):\n",
    "                newScore = self.tokenScoreMap[key] + score\n",
    "                self.tokenScoreMap[key] =  newScore\n",
    "            else:\n",
    "                self.tokenScoreMap[key] = score                \n",
    "                \n",
    "        for key in list(kkcMap.keys()):\n",
    "            score = kkcMap[key]\n",
    "            score = score * psi1\n",
    "            if key in list(self.tokenScoreMap.keys()):\n",
    "                newScore = self.tokenScoreMap[key] + score\n",
    "                self.tokenScoreMap[key] = newScore\n",
    "            else:\n",
    "                self.tokenScoreMap[key] = score\n",
    "                \n",
    "        # return rankAPIElements(tokenScoreMap);\n",
    "    \n",
    "    def addCombinedRankingsV2(self, kacMap, kpacMap, kkcMap, alpha1, beta1, psi1):\n",
    "        # get the combined rankings\n",
    "        self.tokenScoreMap = {}\n",
    "        # HashMap<String, Double> tokenScoreMap = new HashMap<>()\n",
    "        for key in kacMap.keys():\n",
    "            score = kacMap[key]\n",
    "            score = score * alpha1\n",
    "            if key in list(self.tokenScoreMap.keys()):\n",
    "                newScore = max(self.tokenScoreMap[key] , score)\n",
    "                self.tokenScoreMap[key] = newScore\n",
    "            else:\n",
    "                self.tokenScoreMap[key] = score\n",
    "                \n",
    "        for key in list(kpacMap.keys()):\n",
    "            score = kpacMap[key]\n",
    "            score = score * beta1\n",
    "            if key in list(self.tokenScoreMap.keys()):\n",
    "                newScore = max(self.tokenScoreMap[key] , score)\n",
    "                self.tokenScoreMap[key] =  newScore\n",
    "            else:\n",
    "                self.tokenScoreMap[key] = score                \n",
    "                \n",
    "        for key in list(kkcMap.keys()):\n",
    "            score = kkcMap[key]\n",
    "            score = score * psi1\n",
    "            if key in list(self.tokenScoreMap.keys()):\n",
    "                newScore = max(self.tokenScoreMap[key] , score)\n",
    "                self.tokenScoreMap[key] = newScore\n",
    "            else:\n",
    "                self.tokenScoreMap[key] = score\n",
    "                \n",
    "        # return rankAPIElements(tokenScoreMap)\n",
    "        \n",
    "    def getNormScore(self, apis):\n",
    "        tempMap = {}\n",
    "        index = 0\n",
    "        for api in apis:\n",
    "            index = index + 1\n",
    "            score = 1 - index / len(apis)\n",
    "            tempMap[api] = score\n",
    "            # index++\n",
    "        \n",
    "        return tempMap\n",
    "    \n",
    "    def recommendRelevantAPIs(self, key):\n",
    "        # recommend API names for a query\n",
    "        queryTerms = self.decomposeQueryTerms()\n",
    "        # collecting scores\n",
    "        if len(key) > 0:\n",
    "            if key[0] == \"KAC\":\n",
    "                self.collectTokenScoresKAC(queryTerms)\n",
    "            elif key[0] == \"KPAC\":\n",
    "                self.collectTokenScoresKPAC(queryTerms)\n",
    "            elif key[0] == \"KKC\":\n",
    "                self.collectTokenScoresKKC(queryTerms)\n",
    "            elif key[0] == \"all\":\n",
    "                self.collectTokenScores(queryTerms)\n",
    "            else:\n",
    "                self.collectTokenScores(queryTerms)\n",
    "        else:\n",
    "            self.collectTokenScores(queryTerms)\n",
    "            \n",
    "        apis = self.rankAPIElements()\n",
    "        \n",
    "        # now refine the list\n",
    "        # lets keep the duplicates\n",
    "        # apis = discardDuplicates(apis)\n",
    "        \n",
    "        # now normalize the component scores\n",
    "        self.KACMap = self.normalizeMapScores(self.KACMap)\n",
    "        self.KPACMap = self.normalizeMapScores(self.KPACMap)\n",
    "        self.KKCMap = self.normalizeMapScores(self.KKCMap);\n",
    "\n",
    "        # now demonstrate the API\n",
    "        resultAPIs = []\n",
    "        suggestedResults = []\n",
    "\n",
    "        for api in apis:\n",
    "            if api.strip() == '':\n",
    "                continue;\n",
    "            \n",
    "            # adding the results with scores\n",
    "            atoken = APIToken()\n",
    "            atoken.token = api\n",
    "            if api in self.KACMap.keys():\n",
    "                atoken.KACScore = self.KACMap[api]\n",
    "            if api in self.KPACMap.keys():\n",
    "                atoken.KPACScore = self.KPACMap[api]\n",
    "            if api in self.KKCMap.keys():\n",
    "                atoken.KKCScore = self.KKCMap[api]\n",
    "            if api in self.tokenScoreMap.keys():\n",
    "                atoken.totalScore = self.tokenScoreMap[api]\n",
    "                \n",
    "            suggestedResults.append(atoken)\n",
    "            resultAPIs.append(api)\n",
    "            \n",
    "            if len(resultAPIs) == MAXAPI:\n",
    "                break\n",
    "            \n",
    "        # showAPIs(apis)\n",
    "        return suggestedResults\n",
    "    \n",
    "    def showAPIs(self, apis):\n",
    "        print(apis)\n",
    "        \n",
    "    def normalizeMapScores(self, tempScoreMap):\n",
    "        maxScore = 0\n",
    "        for api in tempScoreMap.keys():\n",
    "            score = tempScoreMap[api]\n",
    "            if score > maxScore:\n",
    "                maxScore = score\n",
    "                \n",
    "        for api in tempScoreMap.keys():\n",
    "            myscore = tempScoreMap[api]\n",
    "            normScore = myscore / maxScore\n",
    "            tempScoreMap[api] = normScore\n",
    "\n",
    "        return tempScoreMap\n",
    "    \n",
    "    def discardDuplicates(self, results):\n",
    "        return list(set(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and Successfully Connected to SQLite\n",
      "SQLite Database Version is:  [('3.32.3',)]\n",
      "Database created and Successfully Connected to SQLite\n",
      "SQLite Database Version is:  [('3.32.3',)]\n",
      "[[-1, 0.19439204, 0.004432883], [0.19439204, -1, 0.05976548], [0.004432883, 0.05976548, -1]]\n"
     ]
    }
   ],
   "source": [
    "# driver for ConnectionManager\n",
    "\n",
    "conn = ConnectionManager()\n",
    "sqliteConnection = conn.getConnection()\n",
    "if sqliteConnection:\n",
    "    sqliteConnection.close()\n",
    "\n",
    "# driver for AdjacencyScoreProvider\n",
    "provider = AdjacencyScoreProvider(['extract','method','class'])\n",
    "provider.getQueryTermAdjacencyScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extract', 'methodifi', 'classifi']\n"
     ]
    }
   ],
   "source": [
    "# driver for TokenStemmer\n",
    "tokStem = TokenStemmer()\n",
    "print(tokStem.performStemming(['extracting', 'methodify', 'classify']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nick likes play football however fond tennis\n"
     ]
    }
   ],
   "source": [
    "# driver for StopWordRemover\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "swr = StopWordRemover()\n",
    "print(swr.getRefinedSentence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and Successfully Connected to SQLite\n",
      "SQLite Database Version is:  [('3.32.3',)]\n",
      "{'extract': ['Pattern', 'Matcher', 'File', 'ArrayList', 'Document', 'Element', 'IOException', 'InputStream', 'Map', 'Jsoup'], 'method': ['Object', 'Class', 'ArrayList', 'Test', 'Method', 'Map', 'Foo', 'Arrays', 'HashMap', 'MyClass'], 'class': ['Class', 'Object', 'ArrayList', 'Test', 'Foo', 'Map', 'File', 'Set', 'Main', 'MyClass']}\n"
     ]
    }
   ],
   "source": [
    "# driver for RelevantAPICollector\n",
    "apiCollector = RelevantAPICollector(['extract','method','class'])\n",
    "print(apiCollector.collectAPIsforQuery())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and Successfully Connected to SQLite\n",
      "SQLite Database Version is:  [('3.32.3',)]\n",
      "{'File': 0.744186046511628, 'FileOutputStream': 0.558139534883721, 'IOException': 0.372093023255814, 'InputStream': 1.0, 'Files': 0.9767441860465117, 'FileInputStream': 0.3255813953488372, 'Path': 0.6046511627906977, 'OutputStream': 0.7906976744186047, 'FileChannel': 0.20930232558139533, 'BufferedReader': 0.04651162790697674, 'Java': 0.372093023255814, 'Sun': 0.23255813953488372, 'VolcanoRobot': 0.04651162790697674}\n"
     ]
    }
   ],
   "source": [
    "# driver for CoocurrenceScoreProvider\n",
    "queryTerms = []\n",
    "queryTerms.append(\"copi\")\n",
    "queryTerms.append(\"file\")\n",
    "queryTerms.append(\"jdk\")\n",
    "print(CoocurrenceScoreProvider(queryTerms).getCoocScores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and Successfully Connected to SQLite\n",
      "SQLite Database Version is:  [('3.32.3',)]\n",
      "Database created and Successfully Connected to SQLite\n",
      "SQLite Database Version is:  [('3.32.3',)]\n",
      "Database created and Successfully Connected to SQLite\n",
      "SQLite Database Version is:  [('3.32.3',)]\n",
      "Class 0.3684210526315789 0 0 0.0\n",
      "JSONObject 0.4210526315789474 0.2222222222222222 0 0.0\n",
      "File 0.7368421052631579 0.48148148148148145 1.0 0.0\n",
      "IOException 0.7894736842105263 0.6296296296296295 1.0 0.0125\n",
      "Elements 0.3684210526315789 0.5185185185185185 0 0.032499999999999994\n",
      "Node 0 0.2222222222222222 0 0.05749999999999998\n",
      "Element 0.5789473684210527 0.5555555555555555 0.118169226 0.0625\n",
      "Pattern 0.3157894736842105 0.48148148148148145 0.118169226 0.07500000000000001\n",
      "Document 1.0 1.0 0.118169226 0.08750000000000001\n",
      "JFrame 0.2105263157894737 0 0 0.09750000000000002\n"
     ]
    }
   ],
   "source": [
    "# driver for CodeTokenProvider\n",
    "query = \"How to parse HTML in Java?\"\n",
    "provider = CodeTokenProvider(query)\n",
    "results = provider.recommendRelevantAPIs(\"all\")\n",
    "for atoken in results:\n",
    "    print(atoken.token + \" \" + str(atoken.KACScore) + \" \" + str(atoken.KPACScore) + \" \" + str(atoken.KKCScore) + \" \" + str(atoken.totalScore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
